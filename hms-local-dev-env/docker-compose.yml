networks:
  hms-network:
    driver: bridge

services:
  # -----------------------------------------------
  # 1. INFRASTRUCTURE (Postgres, Kafka, Redis, etc.)
  # -----------------------------------------------
  
  postgres:
    image: postgres:15
    container_name: postgres
    environment:
      - POSTGRES_USER=postgres
      - POSTGRES_PASSWORD=postgres
    ports:
      - "5432:5432" # Expose to host for debugging
    networks:
      - hms-network
    volumes:
      - postgres_data:/var/lib/postgresql/data
      # Initialize databases on first startup
      - ./postgres/init:/docker-entrypoint-initdb.d:ro
    healthcheck:
      test: ["CMD-SHELL", "pg_isready -U postgres"]
      interval: 5s
      timeout: 5s
      retries: 5
      start_period: 10s

  redis:
    image: redis:7-alpine
    container_name: redis
    ports:
      - "6379:6379"
    networks:
      - hms-network
    healthcheck:
      test: ["CMD", "redis-cli", "ping"]
      interval: 5s
      timeout: 3s
      retries: 5
      start_period: 5s

  zookeeper:
    image: confluentinc/cp-zookeeper:7.5.0
    container_name: zookeeper
    environment:
      ZOOKEEPER_CLIENT_PORT: 2181
      ZOOKEEPER_TICK_TIME: 2000
    networks:
      - hms-network

  kafka:
    image: confluentinc/cp-kafka:7.5.0
    container_name: kafka
    depends_on:
      - zookeeper
    ports:
      - "9093:9092" # Changed to 9093 to avoid conflict with Electron process on 9092
    environment:
      KAFKA_BROKER_ID: 1
      KAFKA_ZOOKEEPER_CONNECT: zookeeper:2181
      KAFKA_ADVERTISED_LISTENERS: PLAINTEXT://kafka:9092,PLAINTEXT_HOST://localhost:9093
      KAFKA_LISTENER_SECURITY_PROTOCOL_MAP: PLAINTEXT:PLAINTEXT,PLAINTEXT_HOST:PLAINTEXT
      KAFKA_INTER_BROKER_LISTENER_NAME: PLAINTEXT
      KAFKA_OFFSETS_TOPIC_REPLICATION_FACTOR: 1
    networks:
      - hms-network

  # Debezium Connect (for Transactional Outbox pattern)
  debezium-connect:
    image: debezium/connect:2.5
    container_name: debezium-connect
    depends_on:
      - kafka
      - postgres
    ports:
      - "8083:8083"
    environment:
      BOOTSTRAP_SERVERS: kafka:9092
      GROUP_ID: 1
      CONFIG_STORAGE_TOPIC: my_connect_configs
      OFFSET_STORAGE_TOPIC: my_connect_offsets
      STATUS_STORAGE_TOPIC: my_connect_statuses
      CONFIG_STORAGE_REPLICATION_FACTOR: 1
      OFFSET_STORAGE_REPLICATION_FACTOR: 1
      STATUS_STORAGE_REPLICATION_FACTOR: 1
    networks:
      - hms-network

  # Kuma Service Mesh Control Plane (Stable 2.8.3 LTS)
  # Using OSS kumahq image (not kong/kuma-cp Enterprise)
  kuma-cp:
    image: kumahq/kuma-cp:2.8.3
    container_name: kuma-cp
    command: run  # CRITICAL: Explicitly run the control plane process
    ports:
      - "5681:5681" # REST API (GUI accessible at /gui/)
      - "5682:5682" # HTTPS API (TLS)
      - "5683:5683" # Inter-CP Server (TLS, for multi-zone)
    environment:
      # CORRECTED: 'standalone' is the correct mode for a single cluster
      # 'universal' is NOT a valid value for KUMA_MODE
      KUMA_MODE: "standalone"
      # 'universal' is the correct environment for Docker/non-Kubernetes
      KUMA_ENVIRONMENT: "universal"
      KUMA_STORE_TYPE: "memory"
      # Allow sidecars to join without tokens (Standard for Local Dev)
      # In 2.8.3, use dpProxy type none to disable token auth
      KUMA_DP_SERVER_AUTHN_DP_PROXY_TYPE: "none"
      # TLS Certificate for DP Server (bootstrap server on port 5678)
      # Must include "kuma-cp" as a SAN for Docker DNS resolution
      # Note: Kuma 2.8.3 uses KUMA_GENERAL_TLS_* for all TLS configs
      KUMA_GENERAL_TLS_CERT_FILE: /kuma-certs/kuma-cp.crt
      KUMA_GENERAL_TLS_KEY_FILE: /kuma-certs/kuma-cp.key
    volumes:
      # Mount generated TLS certificates (includes kuma-cp hostname)
      - ./kuma/kuma-cp.crt:/kuma-certs/kuma-cp.crt:ro
      - ./kuma/kuma-cp.key:/kuma-certs/kuma-cp.key:ro
    networks:
      - hms-network
    # Removed internal healthcheck because minimal images lack curl/wget.
    # The ./init-kuma.sh script performs the external health check.

  # Kong Gateway (The API Gateway - Ingress Layer)
  kong:
    image: kong:3.4
    container_name: kong
    ports:
      - "8000:8000" # The Public Proxy Port (Use this instead of direct service ports)
      - "8001:8001" # The Admin API (For debugging and management)
    environment:
      KONG_DATABASE: "off" # DB-less mode (Declarative Configuration)
      KONG_DECLARATIVE_CONFIG: /usr/local/kong/declarative/kong.yml
      KONG_PROXY_ACCESS_LOG: /dev/stdout
      KONG_ADMIN_ACCESS_LOG: /dev/stdout
      KONG_PROXY_ERROR_LOG: /dev/stderr
      KONG_ADMIN_ERROR_LOG: /dev/stderr
      KONG_ADMIN_LISTEN: 0.0.0.0:8001
      KONG_PROXY_LISTEN: 0.0.0.0:8000
    volumes:
      - ./kong/kong.yml:/usr/local/kong/declarative/kong.yml:ro
    networks:
      - hms-network
    depends_on:
      kuma-cp:
        condition: service_started
      hms-auth-bff:
        condition: service_started
      hms-onboarding-workflow:
        condition: service_started
    healthcheck:
      test: ["CMD", "kong", "health"]
      interval: 10s
      timeout: 10s
      retries: 5

  # -----------------------------------------------
  # 2. YOUR HMS MICROSERVICES
  # -----------------------------------------------
  
  # The BFF Service
  hms-auth-bff:
    # This tells Docker Compose to build the image from the Dockerfile
    # in the 'hms-auth-bff' repo (which you cloned)
    build:
      context: ../hms-auth-bff # Assumes it's in a sibling folder
    container_name: hms-auth-bff
    ports:
      - "8080:8080" # Exposed for debugging; Production traffic should go through Kong (port 8000)
    environment:
      # Pass in all required secrets/config
      - POSTGRES_USER=postgres
      - POSTGRES_PASSWORD=postgres
      - POSTGRES_HOST=postgres # <-- Uses Docker DNS
      - SCALEKIT_ENVIRONMENT_URL=${SCALEKIT_ENVIRONMENT_URL}
      - SCALEKIT_CLIENT_ID=${SCALEKIT_CLIENT_ID}
      - SCALEKIT_CLIENT_SECRET=${SCALEKIT_CLIENT_SECRET}
      - SCALEKIT_WEBHOOK_SECRET=${SCALEKIT_WEBHOOK_SECRET}
      - SPRING_DATASOURCE_URL=jdbc:postgresql://postgres:5432/bff_db
      - SPRING_REDIS_HOST=redis # <-- Required for distributed sessions (Factor VI)
      - ONBOARDING_URL=http://hms-onboarding-workflow:8080/api/v1
    networks:
      - hms-network
    depends_on:
      kuma-cp:
        condition: service_started
      postgres:
        condition: service_healthy
      redis:
        condition: service_healthy
    healthcheck:
      test: ["CMD-SHELL", "curl -f http://localhost:8080/actuator/health || exit 1"]
      interval: 30s
      timeout: 10s
      retries: 3
      start_period: 60s

  # Kuma Sidecar for BFF Service (Synced to 2.8.3)
  hms-auth-bff-sidecar:
    image: kumahq/kuma-dp:2.8.3
    container_name: hms-auth-bff-sidecar
    # CRITICAL: Use --cp-address to point to kuma-cp (not localhost) since sidecar shares BFF's network
    # Name and mesh are read from the dataplane YAML file, so don't specify them here
    # TOKEN AUTH: Add --dataplane-token-file=/kuma/token after generating tokens (see kuma/README-TOKENS.md)
    # Using HTTPS with certificate that includes kuma-cp hostname
    command: run --cp-address=https://kuma-cp:5678 --dataplane-file=/kuma/dataplane.yaml --dataplane-token-file=/kuma/token
    environment:
      # API Server URL (for dataplane registration)
      KUMA_CONTROL_PLANE_API_SERVER_URL: http://kuma-cp:5681
    volumes:
      - ./kuma/dataplanes/hms-auth-bff.yaml:/kuma/dataplane.yaml:ro
      # TOKEN AUTH: Uncomment after generating token (see kuma/README-TOKENS.md)
      - ./kuma/tokens/hms-auth-bff-token:/kuma/token:ro
    network_mode: "service:hms-auth-bff"
    depends_on:
      - kuma-cp
      - hms-auth-bff

  # The Workflow Service
  hms-onboarding-workflow:
    build:
      context: ../hms-onboarding-workflow
    container_name: hms-onboarding-workflow
    # No ports exposed; this service is purely internal
    environment:
      - POSTGRES_USER=postgres
      - POSTGRES_PASSWORD=postgres
      - POSTGRES_HOST=postgres
      - SCALEKIT_ENVIRONMENT_URL=${SCALEKIT_ENVIRONMENT_URL}
      - SCALEKIT_CLIENT_ID=${SCALEKIT_CLIENT_ID}
      - SCALEKIT_CLIENT_SECRET=${SCALEKIT_CLIENT_SECRET}
      - SPRING_DATASOURCE_URL=jdbc:postgresql://postgres:5432/workflow_db
      - SPRING_KAFKA_BOOTSTRAP_SERVERS=kafka:9092 # <-- Docker DNS
    networks:
      - hms-network
    depends_on:
      kuma-cp:
        condition: service_started
      postgres:
        condition: service_started
      kafka:
        condition: service_started
    healthcheck:
      test: ["CMD-SHELL", "curl -f http://localhost:8080/actuator/health || exit 1"]
      interval: 30s
      timeout: 10s
      retries: 3
      start_period: 60s

  # Kuma Sidecar for Workflow Service (Synced to 2.8.3)
  hms-onboarding-workflow-sidecar:
    image: kumahq/kuma-dp:2.8.3
    container_name: hms-onboarding-workflow-sidecar
    # CRITICAL: Use --cp-address to point to kuma-cp (not localhost) since sidecar shares service's network
    # Name and mesh are read from the dataplane YAML file, so don't specify them here
    # TOKEN AUTH: Add --dataplane-token-file=/kuma/token after generating tokens (see kuma/README-TOKENS.md)
    # Using HTTPS with certificate that includes kuma-cp hostname
    command: run --cp-address=https://kuma-cp:5678 --dataplane-file=/kuma/dataplane.yaml --dataplane-token-file=/kuma/token
    environment:
      # API Server URL (for dataplane registration)
      KUMA_CONTROL_PLANE_API_SERVER_URL: http://kuma-cp:5681
    volumes:
      - ./kuma/dataplanes/hms-onboarding-workflow.yaml:/kuma/dataplane.yaml:ro
      # TOKEN AUTH: Uncomment after generating token (see kuma/README-TOKENS.md)
      - ./kuma/tokens/hms-onboarding-workflow-token:/kuma/token:ro
    network_mode: "service:hms-onboarding-workflow"
    depends_on:
      - kuma-cp
      - hms-onboarding-workflow

  # The CQRS Projector
  hms-dashboard-projector:
    build:
      context: ../hms-dashboard-projector
    container_name: hms-dashboard-projector
    environment:
      - POSTGRES_USER=postgres
      - POSTGRES_PASSWORD=postgres
      - POSTGRES_HOST=postgres
      - SCALEKIT_ENVIRONMENT_URL=${SCALEKIT_ENVIRONMENT_URL}
      - SCALEKIT_CLIENT_ID=${SCALEKIT_CLIENT_ID}
      - SCALEKIT_CLIENT_SECRET=${SCALEKIT_CLIENT_SECRET}
      - SPRING_DATASOURCE_URL=jdbc:postgresql://postgres:5432/projector_db
      - SPRING_KAFKA_BOOTSTRAP_SERVERS=kafka:9092 # <-- Docker DNS
      - SPRING_REDIS_HOST=redis # <-- Docker DNS
    networks:
      - hms-network
    depends_on:
      kuma-cp:
        condition: service_started
      postgres:
        condition: service_started
      kafka:
        condition: service_started
      redis:
        condition: service_started
    healthcheck:
      test: ["CMD-SHELL", "curl -f http://localhost:8080/actuator/health || exit 1"]
      interval: 30s
      timeout: 10s
      retries: 3
      start_period: 60s

  # Kuma Sidecar for Projector Service (Synced to 2.8.3)
  hms-dashboard-projector-sidecar:
    image: kumahq/kuma-dp:2.8.3
    container_name: hms-dashboard-projector-sidecar
    # CRITICAL: Use --cp-address to point to kuma-cp (not localhost) since sidecar shares service's network
    # Name and mesh are read from the dataplane YAML file, so don't specify them here
    # TOKEN AUTH: Add --dataplane-token-file=/kuma/token after generating tokens (see kuma/README-TOKENS.md)
    # Using HTTPS with certificate that includes kuma-cp hostname
    command: run --cp-address=https://kuma-cp:5678 --dataplane-file=/kuma/dataplane.yaml --dataplane-token-file=/kuma/token
    environment:
      # API Server URL (for dataplane registration)
      KUMA_CONTROL_PLANE_API_SERVER_URL: http://kuma-cp:5681
    volumes:
      - ./kuma/dataplanes/hms-dashboard-projector.yaml:/kuma/dataplane.yaml:ro
      # TOKEN AUTH: Uncomment after generating token (see kuma/README-TOKENS.md)
      - ./kuma/tokens/hms-dashboard-projector-token:/kuma/token:ro
    network_mode: "service:hms-dashboard-projector"
    depends_on:
      - kuma-cp
      - hms-dashboard-projector

volumes:
  postgres_data:

